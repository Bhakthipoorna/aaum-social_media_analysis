def InstagramAnalysis(uri,mydb):
	
	import pandas as pd
	import numpy as np
	import nltk
	import os
	import json
	import re
	import pymongo as pym
	from nltk.sentiment.vader import SentimentIntensityAnalyzer
	from nltk.corpus import stopwords
	from nltk.stem.wordnet import WordNetLemmatizer
	sid = SentimentIntensityAnalyzer()
	import psycopg2
	from sqlalchemy import create_engine
	from configparser import ConfigParser
	import datetime
	from dateutil.relativedelta import relativedelta
	import pytz
	IST = pytz.timezone('Asia/Kolkata')
	my_date = datetime.date.today()
	year, week, day_of_week = my_date.isocalendar()
	last_date_of_week = pd.to_datetime(datetime.date(year, 1, 1) + relativedelta(weeks=+week))
	last_date_of_week=str(last_date_of_week)
	last_date_of_week=last_date_of_week[0:10]
	last_date_of_week=pd.to_datetime(last_date_of_week)

	def fetch(uri,mydb):
		uri = uri#"mongodb://localhost:27017"
		client= pym.MongoClient(uri)
		mydb = client['Manual_Data_Push_For_Insta']#client['Insta']
		insta_comments= mydb['Manual_Data_Push_For_Instagram_Comment_Scapper']
		df_insta= pd.json_normalize(insta_comments.find())
		client.close()

		config_file='insta_config.ini'
		config = ConfigParser()
		config.read(config_file)
		cast_names=config['CAST_NAME']['cast_names']

		return df_insta, cast_names

	def transform(df_insta,cast_names):

		df_lyca_production_lookup=pd.read_excel("Insta_PS1_lookup_lyca_productions.xlsx",engine='openpyxl')
		df_madras_talkies_lookup=pd.read_excel("Insta_PS1_lookup_madras_talkies.xlsx",engine='openpyxl')
		df_tips_official_lookup=pd.read_excel("Insta_PS1_lookup_tips_official.xlsx",engine='openpyxl')
		df_jayamravi_lookup=pd.read_excel("Insta_PS1_lookup_jayamravi.xlsx",engine='openpyxl')
		df_aishwaryarai_lookup=pd.read_excel("Insta_PS1_lookup_aishwaryarai.xlsx",engine='openpyxl')
		df_trisha_lookup=pd.read_excel("Insta_PS1_lookup_trisha.xlsx",engine='openpyxl')
		df_karthi_lookup=pd.read_excel("Insta_PS1_lookup_karthi.xlsx",engine='openpyxl')
		df_arrahman_lookup=pd.read_excel("Insta_PS1_lookup_arrahman.xlsx",engine='openpyxl')
		df_manirathnam_lookup=pd.read_excel("Insta_PS1_lookup_maniratnam.xlsx",engine='openpyxl')

		df_lookup=pd.concat([df_lyca_production_lookup,df_madras_talkies_lookup,df_tips_official_lookup,df_jayamravi_lookup,df_aishwaryarai_lookup,df_trisha_lookup,df_karthi_lookup,df_arrahman_lookup,df_manirathnam_lookup],ignore_index=False)

		df_lookup['Image_Url']=df_lookup['Image_Url'].str.strip()
		df_lookup['Image_Url']=df_lookup['Image_Url'].str.lstrip()
		df_lookup['Image_Url']=df_lookup['Image_Url'].str.rstrip()
		df_lookup.drop_duplicates('Image_Url',keep='last',inplace=True)
		df_insta['Image_Url']=df_insta['Image_Url'].str.strip()
		df_insta['Image_Url']=df_insta['Image_Url'].str.lstrip()
		df_insta['Image_Url']=df_insta['Image_Url'].str.rstrip()

		df_lookup_image=df_lookup[~df_lookup['Image_Url'].str.contains('reel/')]

		df_lookup_video=df_lookup[df_lookup['Image_Url'].str.contains('reel/')] # In the lookup page video links contains 'reels/' in the link but the in the collection video links does not contain 'reel/'
		df_lookup_video['Image_Url']=df_lookup_video['Image_Url'].str.replace("reel/","p/") #Ex: 'https://www.instagram.com/p/CfgEMTlDMTo/' in the collection and 'https://www.instagram.com/reels/CfgEMTlDMTo/' in the lookup page

		df_lookup_image['Image_Url']=df_lookup_image['Image_Url'].str[0:-28] # Url in lookup page conatins '?utm_source=ig_web_copy_link' at the end, we are removing this.
		df_lookup_video['Image_Url']=df_lookup_video['Image_Url'].str[0:-28]

		df_picture=df_insta[df_insta['Image_Url'].isin(df_lookup_image['Image_Url'])]
		df_video=df_insta[df_insta['Image_Url'].isin(df_lookup_video['Image_Url'])]
		df_picture=pd.merge(df_picture,df_lookup_image,on='Image_Url',how='left')
		df_video=pd.merge(df_video,df_lookup_video,on='Image_Url',how='left')

		df_picture["Likes Counts"]=df_picture["Likes Counts"].str.lower()
		df_picture['Likes Counts']=df_picture['Likes Counts'].str.strip()
		df_picture['Likes Counts']=df_picture['Likes Counts'].str.lstrip()
		df_picture['Likes Counts']=df_picture['Likes Counts'].str.rstrip()
		df_picture['Likes Counts']=df_picture['Likes Counts'].str[-18:]

		df_picture['Likes Counts']=df_picture['Likes Counts'].str.replace('[^0-9]',' ', flags=re.UNICODE) #(K for thousand and M for million)
		df_video['Likes Counts']=df_video['Likes Counts'].str.replace('[^0-9]',' ', flags=re.UNICODE) #(K for thousand and M for million)

		df_picture['Likes Counts']=df_picture['Likes Counts'].str.replace(" ","")
		df_picture['Likes']=df_picture['Likes Counts']

		df_video['Likes Counts']=df_video['Likes Counts'].str.strip()
		df_video['Likes Counts']=df_video['Likes Counts'].str.lstrip()
		df_video['Likes Counts']=df_video['Likes Counts'].str.rstrip()
		df_video['Likes Counts']=df_video['Likes Counts'].str.replace(" ","")
		df_video['Likes Counts']=df_video['Likes Counts'].str[-18:]
		df_video['Views']=df_video['Likes Counts']

		df=pd.concat([df_picture,df_video],ignore_index=False)

		df_cast_picture_data=df_picture[df_picture['Name of the Page'].str.contains(cast_names)]
		df_cast_video_data=df_video[df_video['Name of the Page'].str.contains(cast_names)]
		df_cast_data=pd.concat([df_cast_picture_data,df_cast_video_data],ignore_index=False)

		df['Date of Post']=pd.to_datetime(df['Date of Post'])
		df['Likes']=df['Likes'].fillna(0)
		df['Views']=df['Views'].fillna(0)
		df['Comments Counts']=df['Comments Counts'].fillna(0)

		df_post_count=pd.DataFrame([[last_date_of_week,df['Image_Url'].nunique()]],columns=["date","posts_count"])

		#summary table by post
		df_summary=pd.DataFrame(df.groupby(['Image_Url','url_description'])['Likes','Views','Comments Counts'].max()).reset_index()
		df_summary.rename(columns={'Comments Counts':'Comments'},inplace=True)
		df_summary['date']=last_date_of_week
		df_summary=df_summary[['date','Image_Url',"url_description",'Likes','Views','Comments']]
		df_summary.rename(columns={"Image_Url":"url","Likes":"likes","Views":"views","Comments":"comments"},inplace=True)
		df_summary=df_summary.head(10)
        
		stop_words = set(stopwords.words("english"))
		new_words = ['amp','co', 'http','youtube','http','www','com','href','result ','search','query','result', 'br', 'channel', 'audience','hai', 'sir', 'youtuber']
		stop_words = stop_words.union(new_words)

		corp = []
		for i in range(0, df.shape[0]):
		    #Remove punctuations
		    text = re.sub('[^a-zA-Z]', ' ', str(df['Comments'].iloc[i])) 
		    text=text.lower()
		    ##Convert to list from string
		    text = text.split()
		    ##Lemmatizing
		    lm = WordNetLemmatizer() 
		    text = [lm.lemmatize(word) for word in text if not word in stop_words] 
		    text = " ".join(text)
		    corp.append(text) 
		df['clean_comments'] = np.array(corp)
		df['Comments']=df['Comments'].astype('str')
		df["Comments"]=df["Comments"].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop_words))
		df['Comments']=df['Comments'].str.replace('[^A-Za-z]', ' ', flags=re.UNICODE, regex= True)
		df['Comments']=df['Comments'].str.lower()


		from sklearn.feature_extraction.text import CountVectorizer
		cv=CountVectorizer(max_df=0.7,
		                   stop_words=stop_words, 
		                   ngram_range=(1,1), 
		                   min_df=0.001)
		X=cv.fit_transform(corp)
		vector = cv.transform(corp)

		#Most frequently occuring words
		def get_top_n_words(corpus, n=None):
		    vec = cv.fit(corp)
		    bag_of_words = vec.transform(corp)
		    sum_words = bag_of_words.sum(axis=0) 
		    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
		    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
		    return words_freq[:n]
		top_words = get_top_n_words(corp, n=20)
		top_df = pd.DataFrame(top_words)
		top_df.columns=["word", "freq"]
		top_df['date']=last_date_of_week
		top_df=top_df[['date','word','freq']]
		drop_words=['please','others','other']
		top_df=top_df[~top_df['word'].isin(drop_words)]
		top_df=top_df.head(5)

		df['clean_comments']=df['clean_comments'].astype('str')
		df['senti']=df['clean_comments'].apply(sid.polarity_scores)

		df['compsenti'] = df['senti'].apply(lambda x: x['compound'])
		df['possenti'] = df['senti'].apply(lambda x: x['pos'])
		df['negsenti'] = df['senti'].apply(lambda x: x['neg'])
		df['nps']=df['possenti']-df['negsenti']
		df['date']=df['Date of Post'].dt.date
		df['date']=pd.to_datetime(df['date'])
		df_nps_over_time=pd.DataFrame(df.groupby('date')['nps'].mean()).reset_index()
		df_nps_over_time=df_nps_over_time.sort_values('date')
		df_nps_over_time=df_nps_over_time[df_nps_over_time['date']>='2022-07-01']
        
		#most positive posts
		df_positive_post=(pd.DataFrame(df.groupby(by= ['Image_Url','url_description'])[ 'possenti'].mean().sort_values(ascending = False).head(5))).reset_index()
		df_positive_post.rename(columns={"Image_Url":"url"},inplace=True)
		df_positive_post['date']=last_date_of_week
		df_positive_post=df_positive_post[["date","url","url_description","possenti"]]
		df_positive_post.rename(columns={'possenti':'positive_sentiment'},inplace=True)

		#most negative posts
		df_negative_post=(pd.DataFrame(df.groupby(by= ['Image_Url','url_description'])[ 'negsenti'].mean().sort_values(ascending = False).head())).reset_index()
		df_negative_post.rename(columns={"Image_Url":"url"},inplace=True)
		df_negative_post['date']=last_date_of_week
		df_negative_post=df_negative_post[["date","url","url_description","negsenti"]]
		df_negative_post.rename(columns={'negsenti':'negative_sentiment'},inplace=True)

		#most positive profiles by NPS
		df_nps_per_persons=(pd.DataFrame(df.groupby(by= 'Commented_Person_name')[ 'nps'].mean())).reset_index()
		df_nps_per_persons.columns=["person_name","nps"] 
		df_nps_per_persons['date']=last_date_of_week
		df_nps_per_persons=df_nps_per_persons[["date","person_name","nps"]]
		df_nps_per_person=df_nps_per_persons.sort_values('nps',ascending=False).head(10)
		
		#most active profiles 
		df_most_active_person=pd.DataFrame(df.Commented_Person_name.value_counts().sort_values(ascending=False))
		df_most_active_person=df_most_active_person.reset_index()
		df_most_active_person.rename(columns={'Commented_Person_name':'No. Of comments from the person','index':'person_name'},inplace=True)
		df_most_active_person
		#Most active profile with nps
		df_most_active_perfile_and_positive_nps=(pd.merge(df_most_active_person,df_nps_per_persons,on='person_name',how='outer')).sort_values('No. Of comments from the person',ascending=False)
		df_most_active_perfile_and_positive_nps=df_most_active_perfile_and_positive_nps[df_most_active_perfile_and_positive_nps['nps']>0]
		df_most_active_perfile_and_positive_nps=df_most_active_perfile_and_positive_nps.head(10)
		df_most_active_perfile_and_positive_nps.rename(columns={'No. Of comments from the person':'comments_count'},inplace=True)
		df_most_active_perfile_and_positive_nps=df_most_active_perfile_and_positive_nps[['date','person_name','comments_count','nps']]
		df_most_active_perfile_and_negative_nps=(pd.merge(df_most_active_person,df_nps_per_persons,on='person_name',how='outer')).sort_values(['No. Of comments from the person'],ascending=False)
		df_most_active_perfile_and_negative_nps=df_most_active_perfile_and_negative_nps[df_most_active_perfile_and_negative_nps['nps']<0]
		df_most_active_perfile_and_negative_nps=df_most_active_perfile_and_negative_nps.head(10)
		df_most_active_perfile_and_negative_nps.rename(columns={'No. Of comments from the person':'comments_count'},inplace=True)
		df_most_active_perfile_and_negative_nps=df_most_active_perfile_and_negative_nps[['date','person_name','comments_count','nps']]#add date column

		df_castwise=pd.DataFrame(df_cast_data.groupby(['Name of the Page','Image_Url'])['Likes','Views','Comments Counts'].max()).reset_index()

		df_castwise['Likes']=pd.to_numeric(df_castwise['Likes'], errors='coerce').convert_dtypes()
		df_castwise['Comments Counts']=pd.to_numeric(df_castwise['Comments Counts'], errors='coerce').convert_dtypes()
		df_castwise['Views']=pd.to_numeric(df_castwise['Views'], errors='coerce').convert_dtypes()

		df_cast_image=pd.DataFrame(df_cast_picture_data.groupby(['Name of the Page'])['Image_Url'].nunique()).reset_index()
		df_cast_image.rename(columns={"Image_Url":"image_posts"},inplace=True)
		df_cast_video=pd.DataFrame(df_cast_video_data.groupby(['Name of the Page'])['Image_Url'].nunique()).reset_index()
		df_cast_video.rename(columns={"Image_Url":"video_posts"},inplace=True)
		df_cast_post_count=pd.DataFrame(df_cast_data.groupby(['Name of the Page'])['Image_Url'].nunique()).reset_index()
		df_cast_post_count.rename(columns={"Image_Url":"Related_posts"},inplace=True)
		df_cast_post_counts=pd.merge(df_cast_post_count,df_cast_image,on='Name of the Page',how='outer').merge(df_cast_video,on='Name of the Page',how='outer')
		df_castwise_report=pd.DataFrame(df_castwise.groupby(['Name of the Page'])[['Likes','Views','Comments Counts']].sum()).reset_index()

		df_castwise_summary=pd.merge(df_castwise_report,df_cast_post_counts,on='Name of the Page',how='outer')
		df_castwise_summary=df_castwise_summary[["Name of the Page","Related_posts","image_posts","video_posts","Likes","Views","Comments Counts"]]
		df_castwise_summary.fillna(0,inplace=True)
		df_castwise_summary.rename(columns={"Name of the Page":"cast_name","Related_posts":"related_posts","Likes":"likes","Views":"views","Comments Counts":"comments"},inplace=True)
		cast_modified_names=['Maniratnam','Vikram','Jayam Ravi','Karthi','Aishwarya Rai Bachchan','Trisha Krishnan','Vikram Prabhu','Ravi Varman','A Sreekar Prasad','A R Rahman','B Jeyamohan']
		df_castwise_summary['cast_name'].replace({'maniratnam.official':'Maniratnam','jayamravi_official':'Jayam Ravi','karthi_offl':'Karthi','aishwaryaraibachchan_arb':'Aishwarya Rai Bachchan','trishakrishnan':'Trisha Krishnan','arrahman':'A R Rahman'},inplace=True)
		cast_modified_names=pd.DataFrame(cast_modified_names)
		cast_modified_names.columns=['cast_name']
		df_castwise_summary=pd.merge(cast_modified_names,df_castwise_summary,on='cast_name',how='left')
		df_castwise_summary.fillna(0,inplace=True)

		return df_post_count, df_summary, top_df, df_nps_over_time, df_positive_post, df_negative_post, df_nps_per_person, df_most_active_perfile_and_positive_nps, df_most_active_perfile_and_negative_nps,df_castwise_summary
    
	def pushdata(df_post_count, df_summary, top_df, df_nps_over_time, df_positive_post, df_negative_post, df_nps_per_person, df_most_active_perfile_and_positive_nps, df_most_active_perfile_and_negative_nps,df_castwise_summary):
		
		conn = psycopg2.connect(database="psone_flash_base", user='psone_flash_base', password='psone', host='kalacitra.in')  
		cursor = conn.cursor()
		delete4="DELETE FROM insta_nps"
		cursor.execute(delete4)
		conn.commit()
		cursor.close()
		conn.close()

		engine = create_engine('postgresql://psone_flash_base:psone@kalacitra.in/psone_flash_base')
		df_post_count.to_sql('insta_post_count',engine,if_exists='append',index=False, method="multi")
		df_summary.to_sql('insta_summary',engine,if_exists='append',index=False, method="multi")
		top_df.to_sql('insta_freq_words',engine,if_exists='append',index=False, method="multi")
		df_nps_over_time.to_sql('insta_nps',engine,if_exists='append',index=False, method="multi")
		df_positive_post.to_sql('insta_positive',engine,if_exists='append',index=False, method="multi")
		df_negative_post.to_sql('insta_negative',engine,if_exists='append',index=False, method="multi")
		df_nps_per_person.to_sql('insta_attractive_profile',engine,if_exists='append',index=False, method="multi")
		df_most_active_perfile_and_positive_nps.to_sql('insta_positive_profile',engine,if_exists='append',index=False, method="multi")
		df_most_active_perfile_and_negative_nps.to_sql('insta_negative_profile',engine,if_exists='append',index=False, method="multi")
		df_castwise_summary.to_sql('insta_cast_analysis',engine,if_exists='append',index=False, method="multi")
		engine.dispose()

	df_insta, cast_names = fetch(uri,mydb)
	df_post_count, df_summary, top_df, df_nps_over_time, df_positive_post, df_negative_post, df_nps_per_person, df_most_active_perfile_and_positive_nps, df_most_active_perfile_and_negative_nps, df_castwise_summary = transform(df_insta, cast_names)
	#pushdata(df_post_count, df_summary, top_df, df_nps_over_time, df_positive_post, df_negative_post, df_nps_per_person, df_most_active_perfile_and_positive_nps, df_most_active_perfile_and_negative_nps,df_castwise_summary)
	
	return df_post_count, df_summary, top_df, df_nps_over_time, df_positive_post, df_negative_post, df_nps_per_person, df_most_active_perfile_and_positive_nps, df_most_active_perfile_and_negative_nps, df_castwise_summary
df_post_count, df_summary, top_df, df_nps_over_time, df_positive_post, df_negative_post, df_nps_per_person, df_most_active_perfile_and_positive_nps, df_most_active_perfile_and_negative_nps, df_castwise_summary = InstagramAnalysis(uri,mydb)