def YoutubeAnalysis(uri,mydb,tag):

	import pandas as pd
	import numpy as np
	import nltk
	import pymongo as pym
	import os
	import tweepy as tw
	import json
	import re
	import pytz
	import datetime
	from configparser import ConfigParser
	from nltk.corpus import stopwords
	from nltk.stem.wordnet import WordNetLemmatizer
	from sklearn.feature_extraction.text import CountVectorizer
	from nltk.sentiment.vader import SentimentIntensityAnalyzer
	sid = SentimentIntensityAnalyzer()
	from datetime import datetime, timedelta
	IST = pytz.timezone('Asia/Kolkata')
	import psycopg2
	from sqlalchemy import create_engine

	def fetch(uri,mydb,tag):
		
		uri = uri #"mongodb://localhost:27017"
		client= pym.MongoClient(uri)
		mydb = client[mydb] #client['socialmedia']

		if tag=='Teaser':

			youtube_comments= mydb['youtube_video_comments']
			df = pd.DataFrame(list(youtube_comments.find()))

		elif tag=='Ponni':

			youtube_comments= mydb['youtube_video_songs']
			df = pd.DataFrame(list(youtube_comments.find()))

		elif tag=='Chola':

			youtube_comments= mydb['youtube_video_songs']
			df = pd.DataFrame(list(youtube_comments.find()))

		elif tag=='Trailer':

			youtube_comments= mydb['youtube_video_comments_trailer']
			df = pd.DataFrame(list(youtube_comments.find()))

		client.close()

		return df


	def transform(df):

		if tag=='Teaser':

			df=df[~df['comment_id'].isnull()]
			df['language']=df['video_id']
			df['language'].replace({'9PYNkUTMDW0':'Kannada','mQHLkXn_kHU':'Telugu','LYMhbm2ORoc':'Tamil','-xvUWCszQPM':'Hindi','FhD1qCWNp2w':'Malayalam'},inplace=True)
			df=df[df['language'].isin(['Kannada','Tamil','Malayalam','Telugu','Hindi'])]
			df.drop_duplicates('_id',keep='last',inplace=True)

		elif tag=='Ponni':
			df=df[~df['comment_id'].isnull()]
			df=df[df['search_name'].str.contains('Ponni Nadhi - Lyric Video|Kaveri Se Milne - Lyric Video|Ponge Nadhi - Lyric Video')]
			conditionsOnSearchName = [df['search_name'].str.contains('Tamil'),df['search_name'].str.contains('Hindi'),df['search_name'].str.contains('Kannada'),df['search_name'].str.contains('Telugu'),df['search_name'].str.contains('Malayalam')]
			valuesOnLanguage = ['Tamil','Hindi','Kannada','Telugu','Malayalam']
			df['language'] = np.select(conditionsOnSearchName, valuesOnLanguage)
			df.drop_duplicates('_id',keep='last',inplace=True)

		elif tag=='Chola':
			df=df[~df['comment_id'].isnull()]
			df=df[df['search_name'].str.contains('Chola Chola - Lyric Video')]
			conditionsOnSearchName = [df['search_name'].str.contains('Tamil'),df['search_name'].str.contains('Hindi'),df['search_name'].str.contains('Kannada'),df['search_name'].str.contains('Telugu'),df['search_name'].str.contains('Malayalam')]
			valuesOnLanguage = ['Tamil','Hindi','Kannada','Telugu','Malayalam']
			df['language'] = np.select(conditionsOnSearchName, valuesOnLanguage)
			df.drop_duplicates('_id',keep='last',inplace=True)

		elif tag=='Trailer':
			df=df[~df['comment_id'].isnull()]
			conditionsOnSearchName = [df['search_name'].str.contains('Tamil'),df['search_name'].str.contains('Hindi'),df['search_name'].str.contains('Kannada'),df['search_name'].str.contains('Telugu'),df['search_name'].str.contains('Malayalam')]
			valuesOnLanguage = ['Tamil','Hindi','Kannada','Telugu','Malayalam']
			df['language'] = np.select(conditionsOnSearchName, valuesOnLanguage)
			df.drop_duplicates('_id',keep='last',inplace=True)


		df['updated_at']=pd.to_datetime(df['updated_at'])
		df=df[df['updated_at']>'2022-09-03']
		df=df[df['updated_at']<'2022-09-10']
		df['date']=df['updated_at'].dt.date
		df['date']=pd.to_datetime(df['date'])

		df['comments_text']=df['comments_text'].str.replace('[^A-Za-z]', ' ', flags=re.UNICODE, regex= True)
		df['comments_text']=df['comments_text'].str.lower()

		df['views_count']=pd.to_numeric(df['views_count'], errors='coerce').convert_dtypes()
		df['like_count']=pd.to_numeric(df['like_count'], errors='coerce').convert_dtypes()
		df['comment_count']=pd.to_numeric(df['comment_count'], errors='coerce').convert_dtypes()

		df['views_count'].fillna(0,inplace=True)
		df['like_count'].fillna(0,inplace=True)
		df['comment_count'].fillna(0,inplace=True)

		df_likes_views_comments=pd.DataFrame(df.groupby(['language']).agg({"like_count":np.max,"views_count":np.max,"comment_count":np.max})).reset_index()
		df_likes_views_comments['date']=datetime.now(IST).date().strftime("%Y-%m-%d")
		#df_likes_views_comments['date']='2022-08-20'
		df_likes_views_comments['date']=pd.to_datetime(df_likes_views_comments['date'])
		df_likes_views_comments['tag']=tag
		df_likes=df_likes_views_comments[['date','tag','language','like_count']]
		df_views=df_likes_views_comments[['date','tag','language','views_count']]
		df_comments=df_likes_views_comments[['date','tag','language','comment_count']]

		df_likes_views_comments=pd.DataFrame(df.groupby(['language','date']).agg({"like_count":np.max,"views_count":np.max,"comment_count":np.max})).reset_index()
		df_likes_views_comments['tag']=tag
		df_likes_overtime=df_likes_views_comments[['date','tag','language','like_count']]
		df_views_overtime=df_likes_views_comments[['date','tag','language','views_count']]
		df_comments_overtime=df_likes_views_comments[['date','tag','language','comment_count']]
		df_likes_overtime=df_likes_overtime.sort_values('date')
		df_views_overtime=df_views_overtime.sort_values('date')
		df_comments_overtime=df_comments_overtime.sort_values('date')

		stop_words = set(stopwords.words("english"))
		new_words = ['amp','co', 'http','youtube','http','www','com','href','result ','search','query','result', 'br', 'channel', 'audience','hai', 'sir', 'youtuber','video','film','please','language','ki','ka','p','se','movie','mera','help']
		stop_words = stop_words.union(new_words)

		corp = []
		for i in range(0, df.shape[0]):
			#Remove punctuations
			text = re.sub('[^a-zA-Z]', ' ', str(df['comments_text'].iloc[i])) 
			text=text.lower()
			##Convert to list from string
			text = text.split()
			##Lemmatizing
			lm = WordNetLemmatizer() 
			text = [lm.lemmatize(word) for word in text if not word in stop_words] 
			text = " ".join(text)
			corp.append(text)

		#Removing stopwords from the comments_text column
		df['comments_text']=df['comments_text'].astype('str')
		df["comments_text"]=df["comments_text"].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop_words))

		cv=CountVectorizer(max_df=0.7, stop_words=stop_words, ngram_range=(1,1), min_df=0.001)
		X=cv.fit_transform(corp)
		vector = cv.transform(corp)

		#Most frequently occuring words
		def get_top_n_words(corpus, n=None):
			vec = cv.fit(corp)
			bag_of_words = vec.transform(corp)
			sum_words = bag_of_words.sum(axis=0) 
			words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
			words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
			return words_freq[:n]

		top_words = get_top_n_words(corp, n=20)
		top_df = pd.DataFrame(top_words)
		top_df.columns=["word", "freq"]
		top_df['date']=datetime.now(IST).date().strftime("%Y-%m-%d")
		top_df['tag']=tag
		drop_words=['please','others']
		top_df=top_df[~top_df['word'].isin(drop_words)]
		top_df=top_df.head(5)
		top_df=top_df[['date','tag','word','freq']]
        

		df['comments_text']=df['comments_text'].astype('str')
		df['senti']=df['comments_text'].apply(sid.polarity_scores)

		df['compsenti'] = df['senti'].apply(lambda x: x['compound'])
		df['possenti'] = df['senti'].apply(lambda x: x['pos'])
		df['negsenti'] = df['senti'].apply(lambda x: x['neg'])

		df=df.set_index('updated_at')
		df['nps']= df['possenti']-df['negsenti']
		df_nps = pd.DataFrame(df.groupby(['language','date'])['nps'].mean()).reset_index()
		df_nps['tag']=tag
		df_nps=df_nps[['date','tag','language','nps']]

		#most positive comment
		df_positive=pd.DataFrame(df.groupby(by= ['language','comments_text'])[ 'possenti'].mean().sort_values(ascending = False)).reset_index()

		df_positive=df_positive.sort_values(['language','possenti'],ascending=False)
		df_positive=df_positive.groupby('language').head(5)
		df_positive_telugu=df_positive[df_positive['language']=='Telugu']
		df_positive_tamil=df_positive[df_positive['language']=='Tamil']
		df_positive_hindi=df_positive[df_positive['language']=='Hindi']
		df_positive_kannada=df_positive[df_positive['language']=='Kannada']
		df_positive_malayalam=df_positive[df_positive['language']=='Malayalam']

		df_telugu_positive=list(df_positive_telugu['comments_text'])
		df_tamil_positive=list(df_positive_tamil['comments_text'])
		df_hindi_positive=list(df_positive_hindi['comments_text'])
		df_kannada_positive=list(df_positive_kannada['comments_text'])
		df_malayalam_positive=list(df_positive_malayalam['comments_text'])

		telugu_positive=pd.DataFrame([['Telugu',df_telugu_positive]])
		tamil_positive=pd.DataFrame([['Tamil',df_tamil_positive]])
		hindi_positive=pd.DataFrame([['Hindi',df_hindi_positive]])
		kannada_positive=pd.DataFrame([['Kannada',df_kannada_positive]])
		malayalam_positive=pd.DataFrame([['Malayalam',df_malayalam_positive]])

		df_positive=pd.concat([telugu_positive,tamil_positive,hindi_positive,kannada_positive,malayalam_positive])
		df_positive.columns=['language','positive_words']
		df_positive['date']=datetime.now(IST).date().strftime("%Y-%m-%d")
		#df_positive['date']='2022-08-27'
		df_positive['date']=pd.to_datetime(df_positive['date'])
		df_positive['tag']=tag
		df_positive=df_positive[['date','tag','language','positive_words']] # Language

		df_negative=pd.DataFrame(df.groupby(by= ['language','comments_text'])[ 'negsenti'].mean().sort_values(ascending = False)).reset_index()

		df_negative=df_negative.sort_values(['language','negsenti'],ascending=False)
		df_negative=df_negative.groupby('language').head(5)
		df_negative_telugu=df_negative[df_negative['language']=='Telugu']
		df_negative_tamil=df_negative[df_negative['language']=='Tamil']
		df_negative_hindi=df_negative[df_negative['language']=='Hindi']
		df_negative_kannada=df_negative[df_negative['language']=='Kannada']
		df_negative_malayalam=df_negative[df_negative['language']=='Malayalam']

		df_negative_telugu=list(df_negative_telugu['comments_text'])
		df_negative_tamil=list(df_negative_tamil['comments_text'])
		df_negative_hindi=list(df_negative_hindi['comments_text'])
		df_negative_kannada=list(df_negative_kannada['comments_text'])
		df_negative_malayalam=list(df_negative_malayalam['comments_text'])

		telugu_negative=pd.DataFrame([['Telugu',df_negative_telugu]])
		tamil_negative=pd.DataFrame([['Tamil',df_negative_tamil]])
		hindi_negative=pd.DataFrame([['Hindi',df_negative_hindi]])
		kannada_negative=pd.DataFrame([['Kannada',df_negative_kannada]])
		malayalam_negative=pd.DataFrame([['Malayalam',df_negative_malayalam]])

		df_negative=pd.concat([telugu_negative,tamil_negative,hindi_negative,kannada_negative,malayalam_negative])
		df_negative.columns=['language','negative_words']
		df_negative['date']=datetime.now(IST).date().strftime("%Y-%m-%d")
		#df_negative['date']='2022-08-27'
		df_negative['date']=pd.to_datetime(df_negative['date'])
		df_negative['tag']=tag
		df_negative=df_negative[['date','tag','language','negative_words']] # Language
		df_positive_negative=pd.merge(df_positive,df_negative,on='language')
		df_positive_negative=pd.merge(df_positive,df_negative,on=['date','tag','language'])


		df_personwise_count=pd.DataFrame(df['author_display_name'].value_counts()).reset_index()
		df_personwise_count.columns=['person_name','comments_count']
		df_nps_per_person=pd.DataFrame(df.groupby('author_display_name')['nps'].mean()).reset_index()
		df_nps_per_person.columns=['person_name','nps']
		df_nps_per_person=df_nps_per_person.sort_values('nps')
		df_positive_profile=pd.merge(df_personwise_count,df_nps_per_person,on='person_name')
		df_positive_profile=df_positive_profile.head(10)
		df_positive_profile['date']=datetime.now(IST).date().strftime("%Y-%m-%d")
		df_positive_profile['date']=pd.to_datetime(df_positive_profile['date'])
		df_positive_profile['tag']='Ponni'
		df_positive_profile=df_positive_profile[['date','tag','person_name','comments_count','nps']]
		
		
		return df_views, df_likes, df_comments, df_views_overtime, df_likes_overtime, df_comments_overtime, top_df, df_nps, df_positive_negative, df_positive_profile


	def pushdata(df_views, df_likes, df_comments, df_views_overtime, df_likes_overtime, df_comments_overtime, top_df, df_nps, df_positive, df_negative):
		engine = create_engine('postgresql://psone_flash_base:psone@kalacitra.in/psone_flash_base')
		
		def trucation(df_views, df_likes, df_comments, df_views_overtime, df_likes_overtime, df_comments_overtime, top_df, df_nps, df_positive, df_negative):

			conn = psycopg2.connect(database="psone_flash_base", user='psone_flash_base', password='psone', host='kalacitra.in')  
			cursor = conn.cursor()
	
			delete1="DELETE FROM yt_views_overtime where tag = tag"
			cursor.execute(delete1)
			conn.commit()
			delete2="DELETE FROM yt_likes_overtime where tag = tag"
			cursor.execute(delete2)
			conn.commit()
			delete3="DELETE FROM yt_comments_overtime where tag = tag"
			cursor.execute(delete3)
			conn.commit()
			delete4="DELETE FROM yt_nps where tag = tag"
			cursor.execute(delete4)
			conn.commit()
			cursor.close()
			conn.close()

			df_views.to_sql('yt_views',engine,if_exists='append',index=False, method="multi")
			df_likes.to_sql('yt_likes',engine,if_exists='append',index=False, method="multi")
			df_comments.to_sql('yt_comments',engine,if_exists='append',index=False, method="multi")
			df_views_overtime.to_sql('yt_views_overtime',engine,if_exists='append',index=False, method="multi")
			df_likes_overtime.to_sql('yt_likes_overtime',engine,if_exists='append',index=False, method="multi")
			df_comments_overtime.to_sql('yt_comments_overtime',engine,if_exists='append',index=False, method="multi")
			top_df.to_sql('yt_freq_words',engine,if_exists='append',index=False, method="multi")
			df_nps.to_sql('yt_nps',engine,if_exists='append',index=False, method="multi")
			df_positive.to_sql('yt_positive',engine,if_exists='append',index=False, method="multi")
			df_negative.to_sql('yt_negative',engine,if_exists='append',index=False, method="multi")
			

		def push_to_historical_collection_table(df_views, df_likes, df_comments, df_views_overtime, df_likes_overtime, df_comments_overtime, top_df, df_nps, df_positive, df_negative):

			df_views.to_sql('yt_views_history',engine,if_exists='append',index=False, method="multi")
			df_likes.to_sql('yt_likes_history',engine,if_exists='append',index=False, method="multi")
			df_comments.to_sql('yt_comments_history',engine,if_exists='append',index=False, method="multi")
			df_views_overtime['attime']=datetime.now(IST).date().strftime("%Y-%m-%d")
			df_views_overtime.to_sql('yt_views_overtime_history',engine,if_exists='append',index=False, method="multi")
			df_likes_overtime['attime']=datetime.now(IST).date().strftime("%Y-%m-%d")
			df_likes_overtime.to_sql('yt_likes_overtime_history',engine,if_exists='append',index=False, method="multi")
			df_comments_overtime['attime']=datetime.now(IST).date().strftime("%Y-%m-%d")
			df_comments_overtime.to_sql('yt_comments_overtime_history',engine,if_exists='append',index=False, method="multi")
			top_df.to_sql('yt_freq_words_history',engine,if_exists='append',index=False, method="multi")
			df_nps['attime']=datetime.now(IST).date().strftime("%Y-%m-%d")
			df_nps.to_sql('yt_nps_history',engine,if_exists='append',index=False, method="multi")
			df_positive.to_sql('yt_positive_history',engine,if_exists='append',index=False, method="multi")
			df_negative.to_sql('yt_negative_history',engine,if_exists='append',index=False, method="multi")

		engine.dispose()

		trucation(df_views, df_likes, df_comments, df_views_overtime, df_likes_overtime, df_comments_overtime, top_df, df_nps, df_positive_negative)
		#push_to_historical_collection_table(df_views, df_likes, df_comments, df_views_overtime, df_likes_overtime, df_comments_overtime, top_df, df_nps, df_positive, df_negative)

	df = fetch(uri,mydb,tag)
	df_views, df_likes, df_comments, df_views_overtime, df_likes_overtime, df_comments_overtime, top_df, df_nps, df_positive_negative, df_positive_profile = transform(df)
	#pushdata(df_views, df_likes, df_comments, df_views_overtime, df_likes_overtime, df_comments_overtime, top_df, df_nps, df_positive, df_negative)

	return df_views,df_likes,df_comments,df_views_overtime,df_likes_overtime,df_comments_overtime,top_df,df_nps, df_positive_negative, df_positive_profile
df_views, df_likes, df_comments, df_views_overtime, df_likes_overtime, df_comments_overtime, top_df, df_nps, df_positive_negative, df_positive_profile = YoutubeAnalysis(uri,mydb,tag)